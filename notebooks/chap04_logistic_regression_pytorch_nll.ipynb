{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3f20b9c",
   "metadata": {},
   "source": [
    "# Binary Text Classification with \n",
    "# Logistic Regression Implemented with PyTorch and NLL Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d561a30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# set this variable to a number to be used as the random seed\n",
    "# or to None if you don't want to set a random seed\n",
    "seed = 1234\n",
    "\n",
    "if seed is not None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c29307e",
   "metadata": {},
   "source": [
    "The dataset is divided in two directories called `train` and `test`.\n",
    "These directories contain the training and testing splits of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edb4d0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.7M\r\n",
      "-rw-r--r-- 1 marco marco 882K Jun 11  2011 imdbEr.txt\r\n",
      "-rw-r--r-- 1 marco marco 827K Apr 12  2011 imdb.vocab\r\n",
      "-rw-r--r-- 1 marco marco 4.0K Jun 25  2011 README\r\n",
      "drwxr-xr-x 4 marco marco 4.0K Apr 22 12:23 test\r\n",
      "drwxr-xr-x 5 marco marco 4.0K Apr 22 12:23 train\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh data/aclImdb/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8b7f0b",
   "metadata": {},
   "source": [
    "Both the `train` and `test` directories contain two directories called `pos` and `neg` that contain text files with the positive and negative reviews, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18cb65f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 66M\r\n",
      "-rw-r--r-- 1 marco marco  21M Apr 12  2011 labeledBow.feat\r\n",
      "drwxr-xr-x 2 marco marco 368K Apr 22 12:23 neg\r\n",
      "drwxr-xr-x 2 marco marco 372K Apr 22 12:23 pos\r\n",
      "drwxr-xr-x 2 marco marco 1.5M Apr 22 12:23 unsup\r\n",
      "-rw-r--r-- 1 marco marco  40M Apr 12  2011 unsupBow.feat\r\n",
      "-rw-r--r-- 1 marco marco 599K Apr 12  2011 urls_neg.txt\r\n",
      "-rw-r--r-- 1 marco marco 599K Apr 12  2011 urls_pos.txt\r\n",
      "-rw-r--r-- 1 marco marco 2.4M Apr 12  2011 urls_unsup.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh data/aclImdb/train/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a08c1e",
   "metadata": {},
   "source": [
    "We will now read the filenames of the positive and negative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e63aeacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of positive reviews: 12500\n",
      "number of negative reviews: 12500\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "pos_files = glob('data/aclImdb/train/pos/*.txt')\n",
    "neg_files = glob('data/aclImdb/train/neg/*.txt')\n",
    "\n",
    "print('number of positive reviews:', len(pos_files))\n",
    "print('number of negative reviews:', len(neg_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efbe4f5",
   "metadata": {},
   "source": [
    "Now, we will use a [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to read the text files, tokenize them, acquire a vocabulary from the training data, and encode it in a document-term matrix in which each row represents a review, and each column represents a term in the vocabulary. Each element $(i,j)$ in the matrix represents the number of times term $j$ appears in example $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6c4c300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x74849 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3445861 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# initialize CountVectorizer indicating that we will give it a list of filenames that have to be read\n",
    "cv = CountVectorizer(input='filename')\n",
    "\n",
    "# learn vocabulary and return sparse document-term matrix\n",
    "doc_term_matrix = cv.fit_transform(pos_files + neg_files)\n",
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275f5bf3",
   "metadata": {},
   "source": [
    "Note in the message printed above that the matrix is of shape (25000, 74894).\n",
    "In other words, it has 1,871,225,000 elements.\n",
    "However, only 3,445,861 elements were stored.\n",
    "This is because most of the elements in the matrix are zeros.\n",
    "The reason is that the reviews are short and most words in the english language don't appear in each review.\n",
    "A matrix that only stores non-zero values is called *sparse*.\n",
    "\n",
    "Now we will convert it to a dense numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d2f3029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 74849)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = doc_term_matrix.toarray()\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6225d694",
   "metadata": {},
   "source": [
    "We will also create a numpy array with the binary labels for the reviews.\n",
    "One indicates a positive review and zero a negative review.\n",
    "The label `y_train[i]` corresponds to the review encoded in row `i` of the `X_train` matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "110f877e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training labels\n",
    "y_pos = np.ones(len(pos_files))\n",
    "y_neg = np.zeros(len(neg_files))\n",
    "y_train = np.concatenate([y_pos, y_neg])\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf297533",
   "metadata": {},
   "source": [
    "Now we will initialize our model, in the form of an array of weights `w` of the same size as the number of features in our dataset (i.e., the number of words in the vocabulary acquired by [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)), and a bias term `b`.\n",
    "Both are initialized to zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94f99550",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples, n_features = X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f466a2",
   "metadata": {},
   "source": [
    "Now we will use the logistic regression learning algorithm to learn the values of `w` and `b` from our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0a50203",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(n_features, 2),\n",
    "    nn.LogSoftmax(dim=1),\n",
    ")\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03a2ef78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28870334d14948b0a13f9d82c277b397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 1:   0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8887bcaab7f4ffc8f279fee57fa0cd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 2:   0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d680afc3c5544268d5c809156e18cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 3:   0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d62d5a615a4b1b900151b42c588edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 4:   0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2731033a392d4f618d33b33482672343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 5:   0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0346f5ce0e114170bee3b8051693d2c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 6:   0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94bb0b06d52b49e1bb71e5b993c27f8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 7:   0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b352fbde486d48f8bb21546016dea775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 8:   0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dfd148e6fda4c869d385368ba09579a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 9:   0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01b2a3855bf4f128fd563e82e2e66d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 10:   0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "lr = 1e-2\n",
    "n_epochs = 10\n",
    "\n",
    "#model = nn.Linear(n_features, 1)\n",
    "loss_func = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "indices = np.arange(n_examples)\n",
    "for epoch in range(10):\n",
    "    n_errors = 0\n",
    "    # randomize training examples\n",
    "    np.random.shuffle(indices)\n",
    "    # for each training example\n",
    "    for i in tqdm(indices, desc=f'epoch {epoch+1}'):\n",
    "        x = X_train[i].unsqueeze(0)\n",
    "        y_true = y_train[i].unsqueeze(0)\n",
    "        # make predictions\n",
    "        y_pred = model(x)\n",
    "        # calculate loss\n",
    "        loss = loss_func(y_pred, y_true)\n",
    "        # calculate gradients through back-propagation\n",
    "        loss.backward()\n",
    "        # optimize model parameters\n",
    "        optimizer.step()\n",
    "        # ensure gradients are set to zero\n",
    "        model.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cb18aa",
   "metadata": {},
   "source": [
    "The next step is evaluating the model on the test dataset.\n",
    "Note that this time we use the [`transform()`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.transform) method of the [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), instead of the [`fit_transform()`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit_transform) method that we used above. This is because we want to use the learned vocabulary in the test set, instead of learning a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56fdb2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_files = glob('data/aclImdb/test/pos/*.txt')\n",
    "neg_files = glob('data/aclImdb/test/neg/*.txt')\n",
    "doc_term_matrix = cv.transform(pos_files + neg_files)\n",
    "X_test = doc_term_matrix.toarray()\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_pos = np.ones(len(pos_files))\n",
    "y_neg = np.zeros(len(neg_files))\n",
    "y_test = np.concatenate([y_pos, y_neg])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c578ae75",
   "metadata": {},
   "source": [
    "Using the model is easy: multiply the document-term matrix by the learned weights and add the bias.\n",
    "We use Python's `@` operator to perform the matrix-vector multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b081198",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = torch.argmax(model(X_test), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af8d8bc",
   "metadata": {},
   "source": [
    "Now we print an evaluation of the prediction results using scikit-learn's [`classification_report()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9f53d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_classification_report(y_true, y_pred):\n",
    "    # count true positives, false positives, true negatives, and false negatives\n",
    "    tp = fp = tn = fn = 0\n",
    "    for gold, pred in zip(y_true, y_pred):\n",
    "        if pred == True:\n",
    "            if gold == True:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        else:\n",
    "            if gold == False:\n",
    "                tn += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "    # calculate precision and recall\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    # calculate f1 score\n",
    "    fscore = 2 * precision * recall / (precision + recall)\n",
    "    # calculate accuracy\n",
    "    accuracy = (tp + tn) / len(y_true)\n",
    "    # number of positive labels in y_true\n",
    "    support = sum(y_true)\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1-score\": fscore,\n",
    "        \"support\": support,\n",
    "        \"accuracy\": accuracy,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95ab91ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.8887943503786268,\n",
       " 'recall': 0.83568,\n",
       " 'f1-score': 0.8614192058714386,\n",
       " 'support': 12500.0,\n",
       " 'accuracy': 0.86556}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_classification_report(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
