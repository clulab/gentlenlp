{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72ffb019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random seed: 42\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import set_seed\n",
    "\n",
    "# random seed\n",
    "seed = 42\n",
    "\n",
    "# set random seed\n",
    "if seed is not None:\n",
    "    print(f'random seed: {seed}')\n",
    "    set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efbd23fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_name = 't5-small'\n",
    "dataset_name = 'wmt16'\n",
    "dataset_config_name = 'ro-en'\n",
    "source_lang = 'ro'\n",
    "target_lang = 'en'\n",
    "max_source_length = 1024\n",
    "max_target_length = 128\n",
    "task_prefix = 'translate Romanian to English: '\n",
    "batch_size = 4\n",
    "label_pad_token_id = -100\n",
    "save_steps = 25_000\n",
    "num_beams = 1\n",
    "learning_rate = 1e-3\n",
    "num_train_epochs = 3\n",
    "output_dir = '/media/data2/t5-translation-example'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71fd1635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wmt16 (/home/marco/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/9e0038fe4cc117bd474d2774032cc133e355146ed0a47021b2040ca9db4645c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6977c48fa04b02bd08d7db41be0521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wmt16 = load_dataset(dataset_name, dataset_config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "209ec045",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "config = AutoConfig.from_pretrained(transformer_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(transformer_name, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8edf4f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    # get source sentences and prepend task prefix\n",
    "    sources = [x[source_lang] for x in batch[\"translation\"]]\n",
    "    sources = [task_prefix + x for x in sources]\n",
    "    # tokenize source sentences\n",
    "    output = tokenizer(\n",
    "        sources,\n",
    "        max_length=max_source_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    # get target sentences\n",
    "    targets = [x[target_lang] for x in batch[\"translation\"]]\n",
    "    # tokenize target sentences\n",
    "    labels = tokenizer(\n",
    "        targets,\n",
    "        max_length=max_target_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "    # add targets to output\n",
    "    output[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "638f7ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/marco/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/9e0038fe4cc117bd474d2774032cc133e355146ed0a47021b2040ca9db4645c0/cache-23cc4847e3a6788f.arrow\n",
      "Loading cached processed dataset at /home/marco/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/9e0038fe4cc117bd474d2774032cc133e355146ed0a47021b2040ca9db4645c0/cache-2cd8d8c9a5ee0dc9.arrow\n"
     ]
    }
   ],
   "source": [
    "train_dataset = wmt16['train']\n",
    "eval_dataset = wmt16['validation']\n",
    "column_names = train_dataset.column_names\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize,\n",
    "    batched=True,\n",
    "    remove_columns=column_names,\n",
    ")\n",
    "\n",
    "eval_dataset = eval_dataset.map(\n",
    "    tokenize,\n",
    "    batched=True,\n",
    "    remove_columns=column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69dd97a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[13959, 3871, 29, 12, 1566, 10, 4961, 106, 204...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[19428, 13, 12876, 10, 217, 13687, 7, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[13959, 3871, 29, 12, 1566, 10, 5085, 5840, 49...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[2276, 8843, 138, 13, 13687, 7, 13, 1767, 3823...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[13959, 3871, 29, 12, 1566, 10, 4961, 106, 204...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[19428, 13, 12876, 10, 217, 13687, 7, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[13959, 3871, 29, 12, 1566, 10, 781, 8750, 9, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[781, 2420, 13, 17500, 10, 217, 13687, 7, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[13959, 3871, 29, 12, 1566, 10, 374, 6225, 49,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[11167, 7, 1204, 10, 217, 13687, 7, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610315</th>\n",
       "      <td>[13959, 3871, 29, 12, 1566, 10, 4540, 4031, 9,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[4540, 4031, 9, 7, 1672, 7, 2262, 900, 17, 38,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610316</th>\n",
       "      <td>[13959, 3871, 29, 12, 1566, 10, 2364, 4540, 40...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[242, 4540, 4031, 9, 7, 6, 8, 516, 65, 66, 8, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610317</th>\n",
       "      <td>[13959, 3871, 29, 12, 1566, 10, 2262, 900, 17,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[2262, 900, 17, 641, 65, 46, 3761, 6, 1069, 31...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610318</th>\n",
       "      <td>[13959, 3871, 29, 12, 1566, 10, 3, 25882, 759,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[9810, 157, 31, 7, 516, 92, 3088, 21, 46, 3839...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610319</th>\n",
       "      <td>[13959, 3871, 29, 12, 1566, 10, 18420, 83, 362...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3625, 32, 5788, 35, 15, 3844, 31, 7, 3, 16143...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>610320 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                input_ids  \\\n",
       "0       [13959, 3871, 29, 12, 1566, 10, 4961, 106, 204...   \n",
       "1       [13959, 3871, 29, 12, 1566, 10, 5085, 5840, 49...   \n",
       "2       [13959, 3871, 29, 12, 1566, 10, 4961, 106, 204...   \n",
       "3       [13959, 3871, 29, 12, 1566, 10, 781, 8750, 9, ...   \n",
       "4       [13959, 3871, 29, 12, 1566, 10, 374, 6225, 49,...   \n",
       "...                                                   ...   \n",
       "610315  [13959, 3871, 29, 12, 1566, 10, 4540, 4031, 9,...   \n",
       "610316  [13959, 3871, 29, 12, 1566, 10, 2364, 4540, 40...   \n",
       "610317  [13959, 3871, 29, 12, 1566, 10, 2262, 900, 17,...   \n",
       "610318  [13959, 3871, 29, 12, 1566, 10, 3, 25882, 759,...   \n",
       "610319  [13959, 3871, 29, 12, 1566, 10, 18420, 83, 362...   \n",
       "\n",
       "                                           attention_mask  \\\n",
       "0       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "...                                                   ...   \n",
       "610315  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "610316  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "610317  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "610318  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "610319  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                                   labels  \n",
       "0                [19428, 13, 12876, 10, 217, 13687, 7, 1]  \n",
       "1       [2276, 8843, 138, 13, 13687, 7, 13, 1767, 3823...  \n",
       "2                [19428, 13, 12876, 10, 217, 13687, 7, 1]  \n",
       "3            [781, 2420, 13, 17500, 10, 217, 13687, 7, 1]  \n",
       "4                  [11167, 7, 1204, 10, 217, 13687, 7, 1]  \n",
       "...                                                   ...  \n",
       "610315  [4540, 4031, 9, 7, 1672, 7, 2262, 900, 17, 38,...  \n",
       "610316  [242, 4540, 4031, 9, 7, 6, 8, 516, 65, 66, 8, ...  \n",
       "610317  [2262, 900, 17, 641, 65, 46, 3761, 6, 1069, 31...  \n",
       "610318  [9810, 157, 31, 7, 516, 92, 3088, 21, 46, 3839...  \n",
       "610319  [3625, 32, 5788, 35, 15, 3844, 31, 7, 3, 16143...  \n",
       "\n",
       "[610320 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "022c1d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb252edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric('sacrebleu')\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # get text for predictions\n",
    "    predictions = tokenizer.batch_decode(\n",
    "        preds,\n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "    # replace -100 in labels with pad token\n",
    "    labels = np.where(\n",
    "        labels != -100,\n",
    "        labels,\n",
    "        tokenizer.pad_token_id,\n",
    "    )\n",
    "    # get text for gold labels\n",
    "    references = tokenizer.batch_decode(\n",
    "        labels,\n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "    # metric expects list of references for each prediction\n",
    "    references = [[ref] for ref in references]\n",
    "    \n",
    "    # compute bleu score\n",
    "    results = metric.compute(\n",
    "        predictions=predictions,\n",
    "        references=references,\n",
    "    )\n",
    "    results = {'bleu': results['score']}\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d84261ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    save_steps=save_steps,\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=save_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    num_train_epochs=num_train_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "425c6d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81411c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "last_checkpoint = None\n",
    "if os.path.isdir(output_dir):\n",
    "    last_checkpoint = get_last_checkpoint(output_dir)\n",
    "\n",
    "if last_checkpoint is not None:\n",
    "    print(f'Checkpoint detected, resuming training at {last_checkpoint}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de6f3cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marco/miniconda3/envs/book/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 610320\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 457740\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='457740' max='457740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [457740/457740 5:14:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>1.248500</td>\n",
       "      <td>1.831403</td>\n",
       "      <td>13.156476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>1.174400</td>\n",
       "      <td>1.743581</td>\n",
       "      <td>13.979321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>1.118000</td>\n",
       "      <td>1.697292</td>\n",
       "      <td>14.718017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>1.068800</td>\n",
       "      <td>1.650289</td>\n",
       "      <td>14.942166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125000</td>\n",
       "      <td>1.038200</td>\n",
       "      <td>1.628099</td>\n",
       "      <td>14.984296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150000</td>\n",
       "      <td>0.986700</td>\n",
       "      <td>1.621915</td>\n",
       "      <td>15.411908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175000</td>\n",
       "      <td>0.949700</td>\n",
       "      <td>1.593482</td>\n",
       "      <td>15.777268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200000</td>\n",
       "      <td>0.947100</td>\n",
       "      <td>1.566495</td>\n",
       "      <td>15.372236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225000</td>\n",
       "      <td>0.916800</td>\n",
       "      <td>1.545556</td>\n",
       "      <td>15.787117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250000</td>\n",
       "      <td>0.908100</td>\n",
       "      <td>1.537921</td>\n",
       "      <td>15.864213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275000</td>\n",
       "      <td>0.879300</td>\n",
       "      <td>1.521595</td>\n",
       "      <td>15.957985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300000</td>\n",
       "      <td>0.899900</td>\n",
       "      <td>1.489792</td>\n",
       "      <td>16.551496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325000</td>\n",
       "      <td>0.836300</td>\n",
       "      <td>1.495629</td>\n",
       "      <td>16.620282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350000</td>\n",
       "      <td>0.820400</td>\n",
       "      <td>1.491153</td>\n",
       "      <td>16.471263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375000</td>\n",
       "      <td>0.824400</td>\n",
       "      <td>1.472328</td>\n",
       "      <td>16.799837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400000</td>\n",
       "      <td>0.829800</td>\n",
       "      <td>1.456437</td>\n",
       "      <td>16.881164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425000</td>\n",
       "      <td>0.824200</td>\n",
       "      <td>1.451152</td>\n",
       "      <td>16.877818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450000</td>\n",
       "      <td>0.800300</td>\n",
       "      <td>1.446606</td>\n",
       "      <td>16.838640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1999\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /media/data2/t5-translation-example/checkpoint-25000\n",
      "Configuration saved in /media/data2/t5-translation-example/checkpoint-25000/config.json\n",
      "Model weights saved in /media/data2/t5-translation-example/checkpoint-25000/pytorch_model.bin\n",
      "tokenizer config file saved in /media/data2/t5-translation-example/checkpoint-25000/tokenizer_config.json\n",
      "Special tokens file saved in /media/data2/t5-translation-example/checkpoint-25000/special_tokens_map.json\n",
      "Copy vocab file to /media/data2/t5-translation-example/checkpoint-25000/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1999\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /media/data2/t5-translation-example/checkpoint-50000\n",
      "Configuration saved in /media/data2/t5-translation-example/checkpoint-50000/config.json\n",
      "Model weights saved in /media/data2/t5-translation-example/checkpoint-50000/pytorch_model.bin\n",
      "tokenizer config file saved in /media/data2/t5-translation-example/checkpoint-50000/tokenizer_config.json\n",
      "Special tokens file saved in /media/data2/t5-translation-example/checkpoint-50000/special_tokens_map.json\n",
      "Copy vocab file to /media/data2/t5-translation-example/checkpoint-50000/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1999\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /media/data2/t5-translation-example/checkpoint-75000\n",
      "Configuration saved in /media/data2/t5-translation-example/checkpoint-75000/config.json\n",
      "Model weights saved in /media/data2/t5-translation-example/checkpoint-75000/pytorch_model.bin\n",
      "tokenizer config file saved in /media/data2/t5-translation-example/checkpoint-75000/tokenizer_config.json\n",
      "Special tokens file saved in /media/data2/t5-translation-example/checkpoint-75000/special_tokens_map.json\n",
      "Copy vocab file to /media/data2/t5-translation-example/checkpoint-75000/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1999\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /media/data2/t5-translation-example/checkpoint-100000\n",
      "Configuration saved in /media/data2/t5-translation-example/checkpoint-100000/config.json\n",
      "Model weights saved in /media/data2/t5-translation-example/checkpoint-100000/pytorch_model.bin\n",
      "tokenizer config file saved in /media/data2/t5-translation-example/checkpoint-100000/tokenizer_config.json\n",
      "Special tokens file saved in /media/data2/t5-translation-example/checkpoint-100000/special_tokens_map.json\n",
      "Copy vocab file to /media/data2/t5-translation-example/checkpoint-100000/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1999\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /media/data2/t5-translation-example/checkpoint-125000\n",
      "Configuration saved in /media/data2/t5-translation-example/checkpoint-125000/config.json\n",
      "Model weights saved in /media/data2/t5-translation-example/checkpoint-125000/pytorch_model.bin\n",
      "tokenizer config file saved in /media/data2/t5-translation-example/checkpoint-125000/tokenizer_config.json\n",
      "Special tokens file saved in /media/data2/t5-translation-example/checkpoint-125000/special_tokens_map.json\n",
      "Copy vocab file to /media/data2/t5-translation-example/checkpoint-125000/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1999\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /media/data2/t5-translation-example/checkpoint-150000\n",
      "Configuration saved in /media/data2/t5-translation-example/checkpoint-150000/config.json\n",
      "Model weights saved in /media/data2/t5-translation-example/checkpoint-150000/pytorch_model.bin\n",
      "tokenizer config file saved in /media/data2/t5-translation-example/checkpoint-150000/tokenizer_config.json\n",
      "Special tokens file saved in /media/data2/t5-translation-example/checkpoint-150000/special_tokens_map.json\n",
      "Copy vocab file to /media/data2/t5-translation-example/checkpoint-150000/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1999\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /media/data2/t5-translation-example/checkpoint-175000\n",
      "Configuration saved in /media/data2/t5-translation-example/checkpoint-175000/config.json\n",
      "Model weights saved in /media/data2/t5-translation-example/checkpoint-175000/pytorch_model.bin\n",
      "tokenizer config file saved in /media/data2/t5-translation-example/checkpoint-175000/tokenizer_config.json\n",
      "Special tokens file saved in /media/data2/t5-translation-example/checkpoint-175000/special_tokens_map.json\n",
      "Copy vocab file to /media/data2/t5-translation-example/checkpoint-175000/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1999\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /media/data2/t5-translation-example/checkpoint-200000\n",
      "Configuration saved in /media/data2/t5-translation-example/checkpoint-200000/config.json\n",
      "Model weights saved in /media/data2/t5-translation-example/checkpoint-200000/pytorch_model.bin\n",
      "tokenizer config file saved in /media/data2/t5-translation-example/checkpoint-200000/tokenizer_config.json\n",
      "Special tokens file saved in /media/data2/t5-translation-example/checkpoint-200000/special_tokens_map.json\n",
      "Copy vocab file to /media/data2/t5-translation-example/checkpoint-200000/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1999\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /media/data2/t5-translation-example/checkpoint-225000\n",
      "Configuration saved in /media/data2/t5-translation-example/checkpoint-225000/config.json\n",
      "Model weights saved in /media/data2/t5-translation-example/checkpoint-225000/pytorch_model.bin\n",
      "tokenizer config file saved in /media/data2/t5-translation-example/checkpoint-225000/tokenizer_config.json\n",
      "Special tokens file saved in /media/data2/t5-translation-example/checkpoint-225000/special_tokens_map.json\n",
      "Copy vocab file to /media/data2/t5-translation-example/checkpoint-225000/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1999\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /media/data2/t5-translation-example/checkpoint-250000\n",
      "Configuration saved in /media/data2/t5-translation-example/checkpoint-250000/config.json\n",
      "Model weights saved in /media/data2/t5-translation-example/checkpoint-250000/pytorch_model.bin\n",
      "tokenizer config file saved in /media/data2/t5-translation-example/checkpoint-250000/tokenizer_config.json\n",
      "Special tokens file saved in /media/data2/t5-translation-example/checkpoint-250000/special_tokens_map.json\n",
      "Copy vocab file to /media/data2/t5-translation-example/checkpoint-250000/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1999\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /media/data2/t5-translation-example/checkpoint-275000\n",
      "Configuration saved in /media/data2/t5-translation-example/checkpoint-275000/config.json\n",
      "Model weights saved in /media/data2/t5-translation-example/checkpoint-275000/pytorch_model.bin\n",
      "tokenizer config file saved in /media/data2/t5-translation-example/checkpoint-275000/tokenizer_config.json\n",
      "Special tokens file saved in /media/data2/t5-translation-example/checkpoint-275000/special_tokens_map.json\n",
      "Copy vocab file to /media/data2/t5-translation-example/checkpoint-275000/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1999\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /media/data2/t5-translation-example/checkpoint-300000\n",
      "Configuration saved in /media/data2/t5-translation-example/checkpoint-300000/config.json\n",
      "Model weights saved in /media/data2/t5-translation-example/checkpoint-300000/pytorch_model.bin\n",
      "tokenizer config file saved in /media/data2/t5-translation-example/checkpoint-300000/tokenizer_config.json\n",
      "Special tokens file saved in /media/data2/t5-translation-example/checkpoint-300000/special_tokens_map.json\n",
      "Copy vocab file to /media/data2/t5-translation-example/checkpoint-300000/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1999\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /media/data2/t5-translation-example/checkpoint-325000\n",
      "Configuration saved in /media/data2/t5-translation-example/checkpoint-325000/config.json\n",
      "Model weights saved in /media/data2/t5-translation-example/checkpoint-325000/pytorch_model.bin\n",
      "tokenizer config file saved in /media/data2/t5-translation-example/checkpoint-325000/tokenizer_config.json\n",
      "Special tokens file saved in /media/data2/t5-translation-example/checkpoint-325000/special_tokens_map.json\n",
      "Copy vocab file to /media/data2/t5-translation-example/checkpoint-325000/spiece.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1999\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /media/data2/t5-translation-example/checkpoint-350000\n",
      "Configuration saved in /media/data2/t5-translation-example/checkpoint-350000/config.json\n",
      "Model weights saved in /media/data2/t5-translation-example/checkpoint-350000/pytorch_model.bin\n",
      "tokenizer config file saved in /media/data2/t5-translation-example/checkpoint-350000/tokenizer_config.json\n",
      "Special tokens file saved in /media/data2/t5-translation-example/checkpoint-350000/special_tokens_map.json\n",
      "Copy vocab file to /media/data2/t5-translation-example/checkpoint-350000/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1999\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /media/data2/t5-translation-example/checkpoint-375000\n",
      "Configuration saved in /media/data2/t5-translation-example/checkpoint-375000/config.json\n",
      "Model weights saved in /media/data2/t5-translation-example/checkpoint-375000/pytorch_model.bin\n",
      "tokenizer config file saved in /media/data2/t5-translation-example/checkpoint-375000/tokenizer_config.json\n",
      "Special tokens file saved in /media/data2/t5-translation-example/checkpoint-375000/special_tokens_map.json\n",
      "Copy vocab file to /media/data2/t5-translation-example/checkpoint-375000/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1999\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /media/data2/t5-translation-example/checkpoint-400000\n",
      "Configuration saved in /media/data2/t5-translation-example/checkpoint-400000/config.json\n",
      "Model weights saved in /media/data2/t5-translation-example/checkpoint-400000/pytorch_model.bin\n",
      "tokenizer config file saved in /media/data2/t5-translation-example/checkpoint-400000/tokenizer_config.json\n",
      "Special tokens file saved in /media/data2/t5-translation-example/checkpoint-400000/special_tokens_map.json\n",
      "Copy vocab file to /media/data2/t5-translation-example/checkpoint-400000/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1999\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /media/data2/t5-translation-example/checkpoint-425000\n",
      "Configuration saved in /media/data2/t5-translation-example/checkpoint-425000/config.json\n",
      "Model weights saved in /media/data2/t5-translation-example/checkpoint-425000/pytorch_model.bin\n",
      "tokenizer config file saved in /media/data2/t5-translation-example/checkpoint-425000/tokenizer_config.json\n",
      "Special tokens file saved in /media/data2/t5-translation-example/checkpoint-425000/special_tokens_map.json\n",
      "Copy vocab file to /media/data2/t5-translation-example/checkpoint-425000/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1999\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /media/data2/t5-translation-example/checkpoint-450000\n",
      "Configuration saved in /media/data2/t5-translation-example/checkpoint-450000/config.json\n",
      "Model weights saved in /media/data2/t5-translation-example/checkpoint-450000/pytorch_model.bin\n",
      "tokenizer config file saved in /media/data2/t5-translation-example/checkpoint-450000/tokenizer_config.json\n",
      "Special tokens file saved in /media/data2/t5-translation-example/checkpoint-450000/special_tokens_map.json\n",
      "Copy vocab file to /media/data2/t5-translation-example/checkpoint-450000/spiece.model\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to /media/data2/t5-translation-example\n",
      "Configuration saved in /media/data2/t5-translation-example/config.json\n",
      "Model weights saved in /media/data2/t5-translation-example/pytorch_model.bin\n",
      "tokenizer config file saved in /media/data2/t5-translation-example/tokenizer_config.json\n",
      "Special tokens file saved in /media/data2/t5-translation-example/special_tokens_map.json\n",
      "Copy vocab file to /media/data2/t5-translation-example/spiece.model\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28cb5e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  total_flos               = 33926705GF\n",
      "  train_loss               =     0.9658\n",
      "  train_runtime            = 5:14:15.83\n",
      "  train_samples            =     610320\n",
      "  train_samples_per_second =     97.103\n",
      "  train_steps_per_second   =     24.276\n"
     ]
    }
   ],
   "source": [
    "metrics = train_result.metrics\n",
    "metrics['train_samples'] = len(train_dataset)\n",
    "\n",
    "trainer.log_metrics('train', metrics)\n",
    "trainer.save_metrics('train', metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "011de2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1999\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 01:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        3.0\n",
      "  eval_bleu               =    35.1923\n",
      "  eval_loss               =     1.4452\n",
      "  eval_runtime            = 0:01:50.71\n",
      "  eval_samples            =       1999\n",
      "  eval_samples_per_second =     18.055\n",
      "  eval_steps_per_second   =      4.516\n"
     ]
    }
   ],
   "source": [
    "# https://discuss.huggingface.co/t/evaluation-results-metric-during-training-is-different-from-the-evaluation-results-at-the-end/15401\n",
    "\n",
    "metrics = trainer.evaluate(\n",
    "    max_length=max_target_length,\n",
    "    num_beams=num_beams,\n",
    "    metric_key_prefix='eval',\n",
    ")\n",
    "\n",
    "metrics['eval_samples'] = len(eval_dataset)\n",
    "\n",
    "trainer.log_metrics('eval', metrics)\n",
    "trainer.save_metrics('eval', metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c1e8428",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    'finetuned_from': transformer_name,\n",
    "    'tasks': 'translation',\n",
    "    'dataset_tags': dataset_name,\n",
    "    'dataset_args': dataset_config_name,\n",
    "    'dataset': f'{dataset_name} {dataset_config_name}',\n",
    "    'language': [source_lang, target_lang],\n",
    "}\n",
    "trainer.create_model_card(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c97c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
